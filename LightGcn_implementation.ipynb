{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"âœ… All imports successful | Seeds set to 42\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG (tune here)\n",
    "# ============================================================================\n",
    "\n",
    "config = {\n",
    "    # Model\n",
    "    \"embedding_dim\": 128,  # try 64/128/256\n",
    "    \"n_layers\": 3,         # try 2/3/4\n",
    "\n",
    "    # Training\n",
    "    \"epochs\": 200,         # try 100-300 on T4\n",
    "    \"batch_size\": 4096,    # try 2048/4096/8192\n",
    "    \"lr\": 1e-3,            # try 5e-4, 1e-3, 2e-3\n",
    "    \"reg\": 1e-5,           # try 1e-4, 5e-5, 1e-5, 5e-6\n",
    "    \"n_negs\": 5,           # try 1/5/10\n",
    "\n",
    "    # Validation / early stopping\n",
    "    \"val_ratio\": 0.10,     # most recent 10% per user from train_df\n",
    "    \"min_user_train_for_val\": 6,  # only split users with >= this many train interactions\n",
    "    \"eval_every\": 5,       # validate every N epochs\n",
    "    \"patience\": 30,        # try 20-50\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING & PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ratings = pd.read_csv('ratings.dat', sep='::', engine='python',\n",
    "                      names=['userId', 'movieId', 'rating', 'timestamp'])\n",
    "ratings['rating'] = 1  # Implicit feedback\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "ratings['user'] = user_encoder.fit_transform(ratings['userId'])\n",
    "ratings['item'] = item_encoder.fit_transform(ratings['movieId'])\n",
    "\n",
    "n_users = ratings['user'].nunique()\n",
    "n_items = ratings['item'].nunique()\n",
    "print(f\"ðŸ“Š Users: {n_users} | Items: {n_items} | Interactions: {len(ratings):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 80:20 TRAIN-TEST SPLIT (Per-User Interaction Level)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"80:20 TRAIN-TEST SPLIT (PER USER)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "users_with_5_or_less = 0\n",
    "users_in_test = 0\n",
    "\n",
    "for user_id in range(n_users):\n",
    "    user_data = ratings[ratings['user'] == user_id].sort_values('timestamp')\n",
    "    n_interactions = len(user_data)\n",
    "\n",
    "    if n_interactions > 5:\n",
    "        # Apply 80:20 split for users with >5 interactions\n",
    "        split_idx = int(0.8 * n_interactions)\n",
    "        train_list.append(user_data.iloc[:split_idx])\n",
    "        test_list.append(user_data.iloc[split_idx:])\n",
    "        users_in_test += 1\n",
    "    else:\n",
    "        # Keep all in training for users with â‰¤5 interactions\n",
    "        train_list.append(user_data)\n",
    "        users_with_5_or_less += 1\n",
    "\n",
    "train_df = pd.concat(train_list, ignore_index=True)\n",
    "test_df = pd.concat(test_list, ignore_index=True) if test_list else pd.DataFrame()\n",
    "\n",
    "print(f\"ðŸ“ˆ Train: {len(train_df):,} | Test: {len(test_df):,}\")\n",
    "print(f\"ðŸ‘¥ Users with â‰¤5 interactions (training only): {users_with_5_or_less}\")\n",
    "print(f\"ðŸ‘¥ Users in test set: {users_in_test}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ITEM-AWARE SPLIT FIX (Strategy B: ensure every item appears in train)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ITEM-AWARE SPLIT FIX (ENSURE ALL ITEMS IN TRAIN)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_items_set = set(train_df['item'].unique())\n",
    "cold_items = sorted(set(range(n_items)) - train_items_set)\n",
    "print(f\"ðŸ§Š Items missing in train (cold items): {len(cold_items)}\")\n",
    "\n",
    "moved_rows = 0\n",
    "if len(cold_items) > 0 and len(test_df) > 0:\n",
    "    # Move one (earliest) test interaction per cold item into train\n",
    "    move_idx = []\n",
    "    for it in cold_items:\n",
    "        candidates = test_df[test_df['item'] == it]\n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "        idx = candidates['timestamp'].idxmin()\n",
    "        move_idx.append(idx)\n",
    "\n",
    "    move_idx = sorted(set(move_idx))\n",
    "    moved_rows = len(move_idx)\n",
    "    if moved_rows > 0:\n",
    "        train_df = pd.concat([train_df, test_df.loc[move_idx]], ignore_index=True)\n",
    "        test_df = test_df.drop(index=move_idx).reset_index(drop=True)\n",
    "\n",
    "missing_after = set(range(n_items)) - set(train_df['item'].unique())\n",
    "if len(missing_after) == 0:\n",
    "    print(\"âœ“ All items appear in training after item-aware fix\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Still missing {len(missing_after)} items in train (cold-start remains)\")\n",
    "\n",
    "print(f\"ðŸ” Moved {moved_rows} interactions from test â†’ train\")\n",
    "print(f\"ðŸ“ˆ Updated Train: {len(train_df):,} | Updated Test: {len(test_df):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION SPLIT (from train_df only) - do NOT tune on test\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION SPLIT (FROM TRAIN ONLY)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "val_ratio = float(config[\"val_ratio\"])\n",
    "min_user_train_for_val = int(config[\"min_user_train_for_val\"])\n",
    "\n",
    "train2_list = []\n",
    "val_list = []\n",
    "\n",
    "for user_id in range(n_users):\n",
    "    udata = train_df[train_df['user'] == user_id].sort_values('timestamp')\n",
    "    n = len(udata)\n",
    "\n",
    "    if n >= min_user_train_for_val:\n",
    "        val_n = int(np.ceil(val_ratio * n))\n",
    "        val_n = min(val_n, n - 1)  # keep at least 1 training interaction\n",
    "        if val_n > 0:\n",
    "            train2_list.append(udata.iloc[:-val_n])\n",
    "            val_list.append(udata.iloc[-val_n:])\n",
    "        else:\n",
    "            train2_list.append(udata)\n",
    "    else:\n",
    "        train2_list.append(udata)\n",
    "\n",
    "train_df2 = pd.concat(train2_list, ignore_index=True)\n",
    "val_df = pd.concat(val_list, ignore_index=True) if len(val_list) > 0 else pd.DataFrame()\n",
    "\n",
    "print(f\"ðŸ“ˆ Train2 (used for training): {len(train_df2):,}\")\n",
    "print(f\"ðŸ§ª Val (used for early stopping): {len(val_df):,}\")\n",
    "\n",
    "# Ensure we didn't reintroduce cold items by moving the only occurrence to val\n",
    "missing_items_after_val = sorted(set(range(n_items)) - set(train_df2['item'].unique()))\n",
    "if len(missing_items_after_val) > 0 and len(val_df) > 0:\n",
    "    print(f\"âš ï¸  {len(missing_items_after_val)} items became cold after val holdout; moving 1 val row/item back to train\")\n",
    "    move_back_idx = []\n",
    "    for it in missing_items_after_val:\n",
    "        candidates = val_df[val_df['item'] == it]\n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "        idx = candidates['timestamp'].idxmin()\n",
    "        move_back_idx.append(idx)\n",
    "\n",
    "    move_back_idx = sorted(set(move_back_idx))\n",
    "    if len(move_back_idx) > 0:\n",
    "        train_df2 = pd.concat([train_df2, val_df.loc[move_back_idx]], ignore_index=True)\n",
    "        val_df = val_df.drop(index=move_back_idx).reset_index(drop=True)\n",
    "\n",
    "missing_items_after_val_fix = set(range(n_items)) - set(train_df2['item'].unique())\n",
    "if len(missing_items_after_val_fix) == 0:\n",
    "    print(\"âœ“ All items appear in Train2 after val adjustment\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Still missing {len(missing_items_after_val_fix)} items in Train2 (isolated nodes may remain)\")\n",
    "\n",
    "# Save splits\n",
    "train_df2.to_csv('train_split.csv', index=False)\n",
    "val_df.to_csv('val_split.csv', index=False)\n",
    "test_df.to_csv('test_split.csv', index=False)\n",
    "print(\"\\nðŸ’¾ Saved: train_split.csv, val_split.csv, test_split.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# POST-SPLIT SANITY CHECKS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"POST-SPLIT SANITY CHECKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_users = train_df2['user'].unique()\n",
    "train_items = train_df2['item'].unique()\n",
    "print(f\"âœ“ Training users (Train2): {len(train_users)}/{n_users}\")\n",
    "print(f\"âœ“ Training items (Train2): {len(train_items)}/{n_items}\")\n",
    "\n",
    "val_users = val_df['user'].unique() if len(val_df) > 0 else []\n",
    "val_users_without_train = set(val_users) - set(train_users)\n",
    "if len(val_users_without_train) > 0:\n",
    "    print(f\"âš ï¸  WARNING: {len(val_users_without_train)} val users have no training data!\")\n",
    "else:\n",
    "    print(\"âœ“ All val users have training data\")\n",
    "\n",
    "test_users = test_df['user'].unique() if len(test_df) > 0 else []\n",
    "test_users_without_train = set(test_users) - set(train_users)\n",
    "if len(test_users_without_train) > 0:\n",
    "    print(f\"âš ï¸  WARNING: {len(test_users_without_train)} test users have no training data!\")\n",
    "else:\n",
    "    print(\"âœ“ All test users have training data\")\n",
    "\n",
    "# Verify no leakage between Train2 and Test\n",
    "train_interactions = set(zip(train_df2['user'].values, train_df2['item'].values))\n",
    "test_interactions = set(zip(test_df['user'].values, test_df['item'].values)) if len(test_df) > 0 else set()\n",
    "leakage = train_interactions & test_interactions\n",
    "if len(leakage) > 0:\n",
    "    print(f\"âš ï¸  WARNING: {len(leakage)} interactions leaked from test to train!\")\n",
    "else:\n",
    "    print(\"âœ“ No data leakage detected (Train2-Test disjoint)\")\n",
    "\n",
    "# ============================================================================\n",
    "# ADJACENCY MATRIX CONSTRUCTION (built only from Train2)\n",
    "# ============================================================================\n",
    "\n",
    "def build_adj_matrix(df, n_users, n_items):\n",
    "    \"\"\"Build normalized adjacency: D^(-1/2) * A * D^(-1/2)\"\"\"\n",
    "    user_item_matrix = sp.coo_matrix(\n",
    "        (df['rating'].values, (df['user'].values, df['item'].values)),\n",
    "        shape=(n_users, n_items)\n",
    "    )\n",
    "\n",
    "    R = sp.vstack([\n",
    "        sp.hstack([sp.csr_matrix((n_users, n_users)), user_item_matrix]),\n",
    "        sp.hstack([user_item_matrix.T, sp.csr_matrix((n_items, n_items))])\n",
    "    ])\n",
    "\n",
    "    rowsum = np.array(R.sum(1)).flatten()\n",
    "    # Avoid divide-by-zero warnings for isolated nodes (degree = 0)\n",
    "    d_inv_sqrt = np.zeros_like(rowsum)\n",
    "    nonzero = rowsum > 0\n",
    "    d_inv_sqrt[nonzero] = np.power(rowsum[nonzero], -0.5)\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    norm_adj = d_mat_inv_sqrt.dot(R).dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "    return norm_adj\n",
    "\n",
    "\n",
    "def convert_sp_mat_to_sp_tensor(X):\n",
    "    \"\"\"Convert scipy sparse to PyTorch sparse tensor\"\"\"\n",
    "    X = X.tocoo()\n",
    "    indices = torch.LongTensor([X.row, X.col])\n",
    "    values = torch.FloatTensor(X.data)\n",
    "    tensor = torch.sparse.FloatTensor(indices, values, torch.Size(X.shape))\n",
    "    return tensor.coalesce()  # âœ… CRITICAL: Coalesce for correctness\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUILDING GRAPH ADJACENCY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_adj_matrix = build_adj_matrix(train_df2, n_users, n_items)\n",
    "print(f\"ðŸ”— Adjacency: {train_adj_matrix.shape} | Edges: {train_adj_matrix.nnz:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LIGHTGCN MODEL\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, n_users, n_items, embedding_dim=64, n_layers=3):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "\n",
    "    def forward(self, adj):\n",
    "        \"\"\"\n",
    "        âœ… CORRECT: Graph propagation through multiple layers\n",
    "        Computes layer-wise embeddings and returns their mean\n",
    "        \"\"\"\n",
    "        all_embeddings = torch.cat([self.user_embedding.weight, self.item_embedding.weight], dim=0)\n",
    "        embs = [all_embeddings]\n",
    "\n",
    "        for _ in range(self.n_layers):\n",
    "            all_embeddings = torch.sparse.mm(adj, all_embeddings)\n",
    "            embs.append(all_embeddings)\n",
    "\n",
    "        embs = torch.stack(embs, dim=1).mean(dim=1)\n",
    "        return embs[:self.n_users], embs[self.n_users:]\n",
    "\n",
    "    def get_ego_embeddings(self):\n",
    "        \"\"\"âœ… Return layer-0 embeddings for regularization\"\"\"\n",
    "        return self.user_embedding.weight, self.item_embedding.weight\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL INITIALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ–¥ï¸  Device: {device}\")\n",
    "\n",
    "train_adj_torch = convert_sp_mat_to_sp_tensor(train_adj_matrix).to(device)\n",
    "model = LightGCN(\n",
    "    n_users,\n",
    "    n_items,\n",
    "    embedding_dim=int(config[\"embedding_dim\"]),\n",
    "    n_layers=int(config[\"n_layers\"]),\n",
    ").to(device)\n",
    "\n",
    "print(f\"ðŸ“ Embeddings: {config['embedding_dim']}-dim | Layers: {config['n_layers']}\")\n",
    "print(f\"ðŸ”¢ Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING PREPARATION (from Train2 only)\n",
    "# ============================================================================\n",
    "\n",
    "# Build user -> positive items mapping (Train2 only)\n",
    "user_pos = defaultdict(set)\n",
    "for u, i in zip(train_df2['user'].values, train_df2['item'].values):\n",
    "    user_pos[int(u)].add(int(i))\n",
    "\n",
    "user_item_pairs = np.array(list(zip(train_df2['user'].values, train_df2['item'].values)), dtype=np.int64)\n",
    "\n",
    "print(f\"\\nðŸ“Š Training pairs (Train2): {len(user_item_pairs):,}\")\n",
    "\n",
    "\n",
    "def sample_neg_vectorized(users, n_items, user_pos, n_negs=1):\n",
    "    \"\"\"Fast negative sampling with rejection sampling (O(1) expected time).\n",
    "\n",
    "    Returns:\n",
    "      - (batch,) if n_negs==1\n",
    "      - (batch, n_negs) if n_negs>1\n",
    "    \"\"\"\n",
    "    users = np.asarray(users, dtype=np.int64)\n",
    "    n_negs = int(n_negs)\n",
    "\n",
    "    if n_negs <= 1:\n",
    "        neg_items = []\n",
    "        for u in users:\n",
    "            pos_set = user_pos[int(u)]\n",
    "            while True:\n",
    "                neg = np.random.randint(0, n_items)\n",
    "                if neg not in pos_set:\n",
    "                    neg_items.append(neg)\n",
    "                    break\n",
    "        return np.asarray(neg_items, dtype=np.int64)\n",
    "\n",
    "    neg_items = np.empty((len(users), n_negs), dtype=np.int64)\n",
    "    for row, u in enumerate(users):\n",
    "        pos_set = user_pos[int(u)]\n",
    "        for k in range(n_negs):\n",
    "            while True:\n",
    "                neg = np.random.randint(0, n_items)\n",
    "                if neg not in pos_set:\n",
    "                    neg_items[row, k] = neg\n",
    "                    break\n",
    "    return neg_items\n",
    "\n",
    "\n",
    "def bpr_loss_multi_neg(\n",
    "    users,\n",
    "    pos_items,\n",
    "    neg_items,\n",
    "    user_emb,\n",
    "    item_emb,\n",
    "    user_emb_ego,\n",
    "    item_emb_ego,\n",
    "    reg_weight=1e-4,\n",
    "):\n",
    "    \"\"\"BPR loss with optional multi-negative sampling.\n",
    "\n",
    "    Regularization is applied to layer-0 (ego) embeddings only.\n",
    "\n",
    "    Shapes:\n",
    "      users: (B,)\n",
    "      pos_items: (B,)\n",
    "      neg_items: (B,) or (B, N)\n",
    "    \"\"\"\n",
    "    u_emb = user_emb[users]           # (B, D)\n",
    "    pos_emb = item_emb[pos_items]     # (B, D)\n",
    "\n",
    "    pos_scores = torch.sum(u_emb * pos_emb, dim=1)  # (B,)\n",
    "\n",
    "    if neg_items.dim() == 1:\n",
    "        neg_emb = item_emb[neg_items]  # (B, D)\n",
    "        neg_scores = torch.sum(u_emb * neg_emb, dim=1)  # (B,)\n",
    "        bpr = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10))\n",
    "\n",
    "        # Reg: ego embeddings only\n",
    "        u_ego = user_emb_ego[users]\n",
    "        pos_ego = item_emb_ego[pos_items]\n",
    "        neg_ego = item_emb_ego[neg_items]\n",
    "\n",
    "        reg_loss = reg_weight * (\n",
    "            torch.sum(u_ego ** 2) + torch.sum(pos_ego ** 2) + torch.sum(neg_ego ** 2)\n",
    "        ) / users.shape[0]\n",
    "\n",
    "        return bpr + reg_loss\n",
    "\n",
    "    # Multi-negatives: neg_items is (B, N)\n",
    "    neg_emb = item_emb[neg_items]  # (B, N, D)\n",
    "    u_emb_exp = u_emb.unsqueeze(1)  # (B, 1, D)\n",
    "    neg_scores = torch.sum(u_emb_exp * neg_emb, dim=2)  # (B, N)\n",
    "\n",
    "    pos_scores_exp = pos_scores.unsqueeze(1)  # (B, 1)\n",
    "    bpr = -torch.mean(torch.log(torch.sigmoid(pos_scores_exp - neg_scores) + 1e-10))\n",
    "\n",
    "    # Reg: ego embeddings only\n",
    "    u_ego = user_emb_ego[users]\n",
    "    pos_ego = item_emb_ego[pos_items]\n",
    "    neg_ego = item_emb_ego[neg_items]  # (B, N, D)\n",
    "\n",
    "    reg_loss = reg_weight * (\n",
    "        torch.sum(u_ego ** 2) + torch.sum(pos_ego ** 2) + torch.sum(neg_ego ** 2)\n",
    "    ) / users.shape[0]\n",
    "\n",
    "    return bpr + reg_loss\n",
    "\n",
    "\n",
    "def build_items_dict(df):\n",
    "    \"\"\"user -> set(items)\"\"\"\n",
    "    d = defaultdict(set)\n",
    "    if df is None or len(df) == 0:\n",
    "        return d\n",
    "    for u, i in zip(df['user'].values, df['item'].values):\n",
    "        d[int(u)].add(int(i))\n",
    "    return d\n",
    "\n",
    "\n",
    "def evaluate_topk(user_emb, item_emb, test_items, train_items, top_k=20):\n",
    "    \"\"\"Leakage-free ranking metrics with train exclusion.\n",
    "\n",
    "    test_items: dict[user] -> set(gt items)\n",
    "    train_items: dict[user] -> set(items to mask)\n",
    "    \"\"\"\n",
    "    hits = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "\n",
    "    for user in test_items:\n",
    "        if user >= len(user_emb):\n",
    "            continue\n",
    "\n",
    "        scores = np.dot(item_emb, user_emb[user])\n",
    "\n",
    "        train_mask = list(train_items[user])\n",
    "        if len(train_mask) > 0:\n",
    "            scores[train_mask] = -np.inf\n",
    "\n",
    "        top_items = np.argsort(scores)[::-1][:top_k]\n",
    "        ground_truth = test_items[user]\n",
    "\n",
    "        hits_in_topk = len(set(top_items) & ground_truth)\n",
    "        hits.append(1 if hits_in_topk > 0 else 0)\n",
    "        precisions.append(hits_in_topk / top_k)\n",
    "        recalls.append(hits_in_topk / len(ground_truth) if len(ground_truth) > 0 else 0)\n",
    "\n",
    "        dcg = sum([1.0 / np.log2(i + 2) for i, item in enumerate(top_items) if item in ground_truth])\n",
    "        idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(ground_truth), top_k))])\n",
    "        ndcgs.append(dcg / idcg if idcg > 0 else 0.0)\n",
    "\n",
    "    return {\n",
    "        'HR': float(np.mean(hits)) if len(hits) > 0 else 0.0,\n",
    "        'Precision': float(np.mean(precisions)) if len(precisions) > 0 else 0.0,\n",
    "        'Recall': float(np.mean(recalls)) if len(recalls) > 0 else 0.0,\n",
    "        'NDCG': float(np.mean(ndcgs)) if len(ndcgs) > 0 else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "val_items_dict = build_items_dict(val_df)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "epochs = int(config[\"epochs\"])\n",
    "batch_size = int(config[\"batch_size\"])\n",
    "lr = float(config[\"lr\"])\n",
    "reg = float(config[\"reg\"])\n",
    "n_negs = int(config[\"n_negs\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "eval_every = int(config[\"eval_every\"])\n",
    "patience = int(config[\"patience\"])\n",
    "\n",
    "best_hr = -1.0\n",
    "best_epoch = 0\n",
    "best_state = None\n",
    "patience_counter = 0\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_batches = max(1, len(user_item_pairs) // batch_size)\n",
    "\n",
    "    perm = np.random.permutation(len(user_item_pairs))\n",
    "    user_item_pairs_shuffled = user_item_pairs[perm]\n",
    "\n",
    "    epoch_start = time.perf_counter()\n",
    "\n",
    "    progress = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "    for batch_idx in progress:\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        batch_pairs = user_item_pairs_shuffled[start_idx:end_idx]\n",
    "        users_np = batch_pairs[:, 0]\n",
    "        pos_items_np = batch_pairs[:, 1]\n",
    "\n",
    "        neg_items_np = sample_neg_vectorized(users_np, n_items, user_pos, n_negs=n_negs)\n",
    "\n",
    "        users = torch.LongTensor(users_np).to(device)\n",
    "        pos_items = torch.LongTensor(pos_items_np).to(device)\n",
    "        neg_items = torch.LongTensor(neg_items_np).to(device)\n",
    "\n",
    "        # Forward with current parameters (one forward per batch)\n",
    "        user_emb_propagated, item_emb_propagated = model(train_adj_torch)\n",
    "        user_emb_ego, item_emb_ego = model.get_ego_embeddings()\n",
    "\n",
    "        loss = bpr_loss_multi_neg(\n",
    "            users,\n",
    "            pos_items,\n",
    "            neg_items,\n",
    "            user_emb_propagated,\n",
    "            item_emb_propagated,\n",
    "            user_emb_ego,\n",
    "            item_emb_ego,\n",
    "            reg_weight=reg,\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            progress.set_postfix(loss=f\"{loss.item():.4f}\", avg_loss=f\"{total_loss/(batch_idx+1):.4f}\")\n",
    "\n",
    "    progress.close()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    epoch_dur = time.perf_counter() - epoch_start\n",
    "\n",
    "    do_eval = (len(val_items_dict) > 0) and ((epoch + 1) % eval_every == 0 or epoch == 0)\n",
    "    if do_eval:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            user_emb_eval, item_emb_eval = model(train_adj_torch)\n",
    "            user_emb_eval = user_emb_eval.cpu().numpy()\n",
    "            item_emb_eval = item_emb_eval.cpu().numpy()\n",
    "\n",
    "        val_metrics = evaluate_topk(user_emb_eval, item_emb_eval, val_items_dict, user_pos, top_k=20)\n",
    "        val_hr = val_metrics['HR']\n",
    "\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        eta = (epochs - epoch - 1) * (elapsed / (epoch + 1))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:3d}/{epochs} | Loss: {avg_loss:.4f} | \"\n",
    "            f\"Val HR@20: {val_hr:.4f} | {epoch_dur:.1f}s | ETA: {eta/60:.1f}m\"\n",
    "        )\n",
    "\n",
    "        if val_hr > best_hr:\n",
    "            best_hr = val_hr\n",
    "            best_epoch = epoch + 1\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nâš ï¸  Early stopping at epoch {epoch+1} (patience={patience})\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"\\nâœ… Best model from epoch {best_epoch} (Val HR@20: {best_hr:.4f})\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No validation available; best_state not set. Model is last epoch.\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL EMBEDDINGS (NO NORMALIZATION)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXTRACTING EMBEDDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    user_emb_final, item_emb_final = model(train_adj_torch)\n",
    "\n",
    "user_emb_final = user_emb_final.cpu().numpy()\n",
    "item_emb_final = item_emb_final.cpu().numpy()\n",
    "\n",
    "np.save('gcn_user_embeddings.npy', user_emb_final)\n",
    "np.save('gcn_item_embeddings.npy', item_emb_final)\n",
    "\n",
    "print(f\"ðŸ’¾ Saved raw embeddings (NO normalization)\")\n",
    "print(f\"   User: {user_emb_final.shape} | Item: {item_emb_final.shape}\")\n",
    "print(f\"â±ï¸  Total training time: {(time.perf_counter() - start_time)/60:.2f} min\")\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION (TEST ONLY, ONCE) - RANKING METRICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION - TEST RANKING METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Build test ground truth\n",
    "test_items_dict = build_items_dict(test_df)\n",
    "\n",
    "# Evaluate at multiple K\n",
    "for k in [10, 20, 50]:\n",
    "    metrics = evaluate_topk(user_emb_final, item_emb_final, test_items_dict, user_pos, top_k=k)\n",
    "\n",
    "    print(f\"\\nðŸ“Š Top-{k} Results:\")\n",
    "    print(f\"   HR@{k}:        {metrics['HR']:.4f}\")\n",
    "    print(f\"   Precision@{k}: {metrics['Precision']:.4f}\")\n",
    "    print(f\"   Recall@{k}:    {metrics['Recall']:.4f}\")\n",
    "    print(f\"   NDCG@{k}:      {metrics['NDCG']:.4f}\")\n",
    "\n",
    "print(f\"\\nâœ… Evaluation complete on {len(test_items_dict)} test users\")\n",
    "print(f\"âœ… Users with â‰¤5 interactions excluded from evaluation: {users_with_5_or_less}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ¯ LIGHTGCN COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâœ“ 80:20 train-test split per user (>5 interactions)\")\n",
    "print(\"âœ“ Item-aware fix applied before validation split\")\n",
    "print(\"âœ“ Validation split from train only (no tuning on test)\")\n",
    "print(\"âœ“ Early stopping uses validation HR@20\")\n",
    "print(\"âœ“ Multi-negative BPR supported via n_negs\")\n",
    "print(\"âœ“ Ranking metrics: Precision@K, Recall@K, NDCG@K\")\n",
    "print(\"âœ“ No data leakage (train items masked during evaluation)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
